services:
  - type: web
    name: portfolio-backend
    env: python
    plan: free
    autoDeploy: true
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn app:app --host 0.0.0.0 --port $PORT
    envVars:
      - key: OPENAI_API_KEY
        sync: false
      - key: OPENAI_BASE_URL
        value: https://api.groq.com/openai/v1
      - key: MODEL_NAME
        value: llama-3.1-8b-instant
      - key: DATABASE_URL
        sync: false
      - key: SUPABASE_URL
        sync: false
      - key: SUPABASE_SERVICE_ROLE_KEY
        sync: false
      - key: SUPABASE_RESUME_BUCKET
        value: resumes
      - key: SUPABASE_ARTIFACT_BUCKET
        value: artifacts
      # Set to your frontend origin (e.g., https://your-frontend.onrender.com)
      - key: API_ALLOW_ORIGINS
        value: ""
      # Use the real LLM (Groq free tier)
      - key: LLM_DRY_RUN
        value: "0"

  - type: web
    name: portfolio-frontend
    runtime: static
    rootDir: frontend
    buildCommand: npm install && npm run build
    staticPublishPath: dist
    envVars:
      # Set to your backend base URL (e.g., https://your-backend.onrender.com)
      - key: VITE_API_BASE_URL
        sync: false
